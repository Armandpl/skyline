{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:17,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/calib_images/calib/39.jpg, no corners found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [01:19,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# termination criteria\n",
    "# https://stackoverflow.com/questions/49038464/opencv-calibrate-fisheye-lens-error-ill-conditioned-matrix\n",
    "# removed cv.fisheye.CALIB_CHECK_COND \n",
    "calibration_flags = cv.fisheye.CALIB_RECOMPUTE_EXTRINSIC + + cv.fisheye.CALIB_FIX_SKEW\n",
    "subpix_criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.1) # 0.001\n",
    "\n",
    "CHECKERBOARD = (9,6)\n",
    "CHECKERBOARD_SQUARE_SIDE = 22 # mm\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((1, CHECKERBOARD[0]*CHECKERBOARD[1], 3), np.float32)\n",
    "# https://stackoverflow.com/questions/37310210/camera-calibration-with-opencv-how-to-adjust-chessboard-square-size\n",
    "objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2) * CHECKERBOARD_SQUARE_SIDE\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "for fname in tqdm(Path(\"../data/calib_images/calib/\").iterdir()):\n",
    "    img = cv.imread(str(fname))\n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, CHECKERBOARD, cv.CALIB_CB_ADAPTIVE_THRESH+cv.CALIB_CB_FAST_CHECK+cv.CALIB_CB_NORMALIZE_IMAGE)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret:\n",
    "        objpoints.append(objp)\n",
    "        cv.cornerSubPix(gray,corners,(3,3),(-1,-1),subpix_criteria)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cv.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        # save image w/ chessboard to make sure everything went well\n",
    "        cv.imwrite(f\"../data/calib_images/processed_calib/{fname.name}\", img)\n",
    "    else:\n",
    "        print(f'{fname}, no corners found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 valid images for calibration\n",
      "K=np.array([[807.001663913809, 0.0, 1769.57653582959], [0.0, 806.2636130746708, 1241.9479742423312], [0.0, 0.0, 1.0]])\n",
      "D=np.array([[-0.010379570060940762], [-0.0031983087526026065], [0.0003114367284975261], [-0.0002890556350962741]])\n"
     ]
    }
   ],
   "source": [
    "# calibrate\n",
    "N_OK = len(objpoints)\n",
    "K = np.zeros((3, 3))\n",
    "D = np.zeros((4, 1))\n",
    "rvecs = [np.zeros((1, 1, 3), dtype=np.float64) for _ in range(N_OK)]\n",
    "tvecs = [np.zeros((1, 1, 3), dtype=np.float64) for _ in range(N_OK)]\n",
    "\n",
    "rms, _, _, _, _ = \\\n",
    "    cv.fisheye.calibrate(\n",
    "        objpoints,\n",
    "        imgpoints,\n",
    "        gray.shape[::-1],\n",
    "        K,\n",
    "        D,\n",
    "        rvecs,\n",
    "        tvecs,\n",
    "        calibration_flags,\n",
    "        (cv.TERM_CRITERIA_EPS+cv.TERM_CRITERIA_MAX_ITER, 30, 1e-6)\n",
    "    )\n",
    "print(\"Found \" + str(N_OK) + \" valid images for calibration\")\n",
    "print(\"K=np.array(\" + str(K.tolist()) + \")\")\n",
    "print(\"D=np.array(\" + str(D.tolist()) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the calibration went well: are straight lines straight\n",
    "img = cv.imread(\"../data/calib_images/vp_calib.png\")\n",
    "# img = cv.imread(\"../data/calib_images/pnp_3264x2464.png\")\n",
    "\n",
    "# 1_real.jpg is 1280x720, resize to 2560x1440 and pad to 3264x2464\n",
    "# resize the image to 2560x1440\n",
    "# img = center_crop(img, (2560, 1440))\n",
    "# img = cv.resize(img, (2560, 1440))\n",
    "\n",
    "# # # calculate padding dimensions\n",
    "# pad_y = (3264 - img.shape[1]) // 2  # vertical padding\n",
    "# pad_x = (2464 - img.shape[0]) // 2  # horizontal padding\n",
    "\n",
    "# # padding the image to make it 3264x2464\n",
    "# img = np.pad(\n",
    "#     img, \n",
    "#     ((pad_y, pad_y), (pad_x, pad_x), (0, 0)), \n",
    "#     mode='constant', \n",
    "#     constant_values=0\n",
    "# )\n",
    "new_K = cv.fisheye.estimateNewCameraMatrixForUndistortRectify(K, D, (3264, 2464), np.eye(3), balance=1.0)\n",
    "map1, map2 = cv.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, (3264,2464), cv.CV_16SC2)\n",
    "undistorted_img = cv.remap(img, map1, map2, interpolation=cv.INTER_LINEAR, borderMode=cv.BORDER_CONSTANT)\n",
    "cv.imwrite(\"undistorted_vp.jpg\", undistorted_img)\n",
    "plt.imshow(undistorted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"K_3264x2464.npy\", K)\n",
    "np.save(\"D_3264x2464.npy\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.07001664e+02, 0.00000000e+00, 1.76957654e+03],\n",
       "       [0.00000000e+00, 8.06263613e+02, 1.24194797e+03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Find camera pose\n",
    "alright unsure how to make this work, vp calib seems fine? and we can add roll as augmentations and lezgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnp find camera pose\n",
    "\n",
    "def draw(img, corners, imgpts):\n",
    "    # lines\n",
    "    corner = tuple(corners[0].ravel())\n",
    "    img = cv.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5)\n",
    "    img = cv.line(img, corner, tuple(imgpts[1].ravel()), (0,255,0), 5)\n",
    "    img = cv.line(img, corner, tuple(imgpts[2].ravel()), (0,0,255), 5)\n",
    "\n",
    "    # # cube\n",
    "    # imgpts = np.int32(imgpts).reshape(-1,2)\n",
    "    # # draw ground floor in green\n",
    "    # img = cv.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\n",
    "    # # draw pillars in blue color\n",
    "    # for i,j in zip(range(4),range(4,8)):\n",
    "    #     img = cv.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3)\n",
    "    # # draw top layer in red color\n",
    "    # img = cv.drawContours(img, [imgpts[4:]],-1,(0,0,255),3)\n",
    "    \n",
    "    return img\n",
    "\n",
    "axis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3) * CHECKERBOARD_SQUARE_SIDE # bc squares are 22 mm wide\n",
    "# axis = np.float32([[0,0,0], [0,3,0], [3,3,0], [3,0,0],\n",
    "#                    [0,0,-3],[0,3,-3],[3,3,-3],[3,0,-3] ]) * 22\n",
    "\n",
    "img = cv.imread(\"../data/calib_images/pnp_3264x2464.png\")\n",
    "\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, corners = cv.findChessboardCorners(gray, (9,6),None)\n",
    "if ret == True:\n",
    "    corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),subpix_criteria)\n",
    "    cv.drawChessboardCorners(img, (9,6), corners2, ret)\n",
    "    # Find the rotation and translation vectors.\n",
    "    ret, rvecs, tvecs = cv.fisheye.solvePnP(objp,corners2, K, D)\n",
    "    # rvecs, tvecs = cv.solvePnPRefineVVS(objp, corners2, K, D, rvecs, tvecs)\n",
    "    if ret:\n",
    "        # project 3D points to image plane\n",
    "        imgpts, jac = cv.projectPoints(axis, rvecs, tvecs, K, D)\n",
    "        img = draw(img,corners2.astype(np.uint),imgpts.astype(np.uint))\n",
    "        cv.imwrite(\"pnp.jpg\", img)\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        print(\"pnp didn't work\")\n",
    "else:\n",
    "    print(\"didn't find\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-57.16322434],\n",
       "       [ 53.1469279 ],\n",
       "       [ 75.67984408]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rad2deg(rvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.40850352],\n",
       "       [ 9.36371699],\n",
       "       [58.603831  ]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvecs/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube doesn't look well aligned?\n",
    "# and tvecs z say 5.8 cm when cam is actually about 12cm above ground\n",
    "# this seems off\n",
    "# is is the calibration that's wrong?\n",
    "# a mistake on my end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = np.load(\"K_3264x2464.npy\")\n",
    "D = np.load(\"D_3264x2464.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Distort sim images\n",
    "\n",
    "To make sim images look like real ones we want to:\n",
    "- crop them like the camera/software does\n",
    "- distort them the same way the lens does\n",
    "- match the simulated camera pose to the real camera pose on the car\n",
    "- mask the car hood and lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "from kornia.geometry.calibration import distort_points\n",
    "import numpy as np\n",
    "\n",
    "def distort_image(image: torch.Tensor, K: np.ndarray, D: np.ndarray) -> torch.Tensor:\n",
    "    # Get image shape\n",
    "    h, w = image.size(1), image.size(2)\n",
    "\n",
    "\n",
    "    # Generate grid of points in the image\n",
    "    x = torch.linspace(0, w-1, w)\n",
    "    y = torch.linspace(0, h-1, h)\n",
    "    grid_y, grid_x = torch.meshgrid(y, x)\n",
    "    points = torch.stack([grid_x.t(), grid_y.t()], dim=2)  # t() is for transpose\n",
    "\n",
    "    # Distort the points\n",
    "    # cv2\n",
    "    # distorted_points = cv.fisheye.distortPoints(points.numpy(), K, D)\n",
    "    # distorted_points = torch.from_numpy(distorted_points)\n",
    "\n",
    "    # kornia\n",
    "    K = torch.from_numpy(K)\n",
    "    D = torch.from_numpy(D)\n",
    "    distorted_points = distort_points(points, K, D)\n",
    "\n",
    "    # Apply the mapping using grid_sample. Note that grid_sample expects the grid to be in the range [-1, 1] and the points to be in the format (x, y), so we have to do some conversions\n",
    "    grid = (distorted_points - torch.tensor([[w/2, h/2]])) / torch.tensor([[w/2, h/2]])  # normalization and centering\n",
    "    grid = grid.permute(2, 0, 1).unsqueeze(0)  # add batch dimension\n",
    "    grid = grid.permute(0, 3, 2, 1)  # reorder dimensions\n",
    "\n",
    "    # Interpolate\n",
    "    distorted_image = torch.nn.functional.grid_sample(image.unsqueeze(0), grid.float(), align_corners=False, padding_mode=\"zeros\")\n",
    "\n",
    "    return distorted_image.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 816, 3)\n",
      "(616, 816, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "from torchvision.io import read_image, write_png\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# now can we distort sim images\n",
    "# img = read_image(\"../data/sim_v_real/1_sim.jpg\").float()\n",
    "img = cv.imread(\"../data/sim_v_real/1_tiny.jpg\") # 816x616\n",
    "\n",
    "rescaled_K = np.copy(K)\n",
    "rescaled_K = rescaled_K/4\n",
    "rescaled_K[2][2] = 1\n",
    "\n",
    "print(img.shape)\n",
    "dist = distort_image(img, rescaled_K, D[:, 0], crop_output=False, crop_type=\"middle\")\n",
    "print(dist.shape)\n",
    "cv.imwrite(\"sim_distorted.jpg\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57353866,  0.23593632,  0.00098039, -0.00281957, -0.03686973]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendering 1, 2, 3, 4 in blender with eevee and cycles\n",
    "# coords (x, y, z)\n",
    "# 1: 2.98999 m, 0.472049 m\n",
    "# 2: 0.88 m, 1.51205 m \n",
    "# 3: 0.12 m, 1.51205 m\n",
    "# 4: 6.57 m, 2.69205 m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center crop 3264x2464 images to 2560x1440 (which is how 720p images are cropped)\n",
    "def center_crop(img, dim):\n",
    "\t\"\"\"Returns center cropped image\n",
    "\tArgs:\n",
    "\timg: image to be center cropped\n",
    "\tdim: dimensions (width, height) to be cropped\n",
    "\t\"\"\"\n",
    "\twidth, height = img.shape[1], img.shape[0]\n",
    "\n",
    "\t# process crop width and height for max available dimension\n",
    "\tcrop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
    "\tcrop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0] \n",
    "\tmid_x, mid_y = int(width/2), int(height/2)\n",
    "\tcw2, ch2 = int(crop_width/2), int(crop_height/2) \n",
    "\tcrop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
    "\treturn crop_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
